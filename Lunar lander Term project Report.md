
# reward 설계 이유

reward의 상대적인 값에 Agent가 행동하는 방식이 결정된다. 상대적인 값을 고려하지 않고 reward를 설계했을 때 위의 사실을 많이 느꼈다. 중간 목표 보상이 최종 목표 보상보다 크면, 최종 목표인 착륙 상태에 도달하지 못하고 공중에 떠 중간 목표 보상을 최대화하였다. 이는 learning Rate를 높여도 해결하기 어려운 문제였다. 간헐적으로 착륙했을 때의 큰 보상보다 중간 목표의 보상을 얻으려고 했다. 따라서 상대적인 크기를 고려하여 보상을 설계했다. 추가한 reward는 밀도 보상(Dense Reward), 희소 보상, 도전 과제의 개념을 이용하여 작성하였다. 기본 Shaping 보상은 아래와 같이 정의된다.

## 기본 보상 (Shaping Reward)
$$R^{n}_{shape} = -100\sqrt{x^{2}+y^2}-100\sqrt{v_{x}^2+v_y^2}-100w+10\rm leg_L + 10leg_R$$
$$ R^n_{shape} = R^n_{shape} - R^{n-1}_{shape}$$
좌표, 속도, 각속도가 작아지고 다리가 닿을수록 목표 달성에 유리하므로 Open AI 기본 보상함수를 참고하여 위와 같이 설계했다.

## 중간 목표 보상

- 작은 각도
- 하강( $v_y < 0$ ) 
- 깃발 사이 유지 
- 연료 페널티

에이전트(Agent)에게 빠른 피드백을 주기 위해 밀도 보상을 추가했다. 위의 세 가지 조건을 만족하면서 하강하는 것이 목표 달성에 유리하므로 아래와 같이 중간 목표 보상을 부여했다. 각도는 1도 이하로 유지할 때 보상을 아닐 때 페널티를 부여했다. 상승을 방지하기 위해 수직 속도가 음수일 때 보상을 부여하고, 낮은 높이에서 속도가 작을수록 추가적인 보상을 부여했다. 상승을 방지하기 위해 수직 속도가 0 이상이면 보상을 음수로 초기화했다. 마지막으로 깃발 사이에 위치할 시에 보상을, 그렇지 않을 시 수평 방향의 위치에 비례해 페널티를 주었다.

마지막으로 높이(y 좌표)가 0.5 이상이고 깃발 사이에 있을 때는, 연료 사용의 페널티 배수를 크게 하여 연료를 적게 쓰도록 유도했고, 위의 상황이 아닐 때는 페널티 배수를 작게 하여 연료를 좀 더 쓰더라도 깃발 사이에 들어가도록 하였다.

## 목표 달성 보상, 실패 패널티 (착륙 / game_over)

착륙 시 중간 목표보다 높은 보상을 부여하였고, 추가적인 목표 깃발 사이 위치 시 더 큰 보상을 부여했다. 또한 착륙 전 원점에 가깝고, 속도가 작을수록 큰 보상을 주고 깃발에서 벗어날 시 큰 페널티를 주었다. game_over가 되는 경우, 보상의 합이 음수인 경우가 대부분이라 매우 큰 음수의 페널티를 부여하지는 않았다.

# 네트워크 설계 이유

Qnetwork : 
상태 입력(12) → 히든레이어1(256) → 히든레이어2(128) → 출력(9)

과제인 “Luna Lander”문제가 “OpenAI의 Pendulum”보다 복잡하므로 더 많은 노드가 필요하므로 노드 개수를 늘려 256 x 128개로 설정하였다. hidden layer가 2개이고 state들이 정규화되어 0~1사이의 값이 입력되므로 Relu를활성화 함수로 사용하였다. 

처음에는 512 X 512의 히든레이어를 사용하였으나, 동일한 보상 함수로 학습을 진행했을 때, 수렴에 필요한 에피소드 수가 많이 증가하여 256 X 128의 네트워크를 사용하며 더 다양한 보상함수로 학습 후 결과(착륙/여부)를 비교하였다.

또한 Dropout 사용을 Layer에 추가하면, 무작위로 Network의 일부 뉴런을 비활성화하므로 학습을 불안정하게 만들 거라고 판단하여 사용하지 않았다.